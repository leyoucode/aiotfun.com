---
title: "PicoClaw：只需 10MB 内存就能跑的个人 AI 助手"
description: "忘掉大服务器吧——这个 RISC-V 驱动的 AI 助手在 15 美元的开发板上 1 秒启动，能帮你管邮件、日历和消息。"
date: "2026-02-19"
cover: "https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=800&h=450&fit=crop"
category: "boards"
formatTag: "spotlight"
agent: "scout"
readingTime: 6
lang: "zh"
tags: ["risc-v", "ai-assistant", "embedded", "open-source"]
---

## 15 美元开发板上的 AI 助手

大多数个人 AI 助手至少需要 1GB 内存和一颗快速 CPU 才能运行。PicoClaw 只需不到 10MB。它跑在一块 15 美元的 Sipeed LicheeRV Nano 上——一块搭载算能 SG2002 SoC、仅有 256MB 片上 DDR3 的 RISC-V 单板计算机。这比大家常用来跑同类工具的 Mac mini 便宜了 98%。

## 它到底能干啥？

PicoClaw 是 OpenClaw 个人 AI 助手的精简分支，用 Go 从零重写，通过自举流程完成。虽然体积极小，但该有的功能一样不少：

- **邮件管理**——清理收件箱、撰写和发送回复
- **日历**——安排会议、设置提醒
- **航班值机**——没看错，真的可以
- **消息通讯**——支持 WhatsApp、Telegram、Discord
- **网络搜索**——集成 Brave Search 实时查询
- **LLM 集成**——连接外部 LLM 提供商进行推理

所有功能打包成一个可执行文件，RISC-V、ARM、x86 通吃。

## 数据相当炸裂

OpenClaw 原版代码超过 43 万行，需要 1GB 以上内存。PicoClaw 用大约 4000 行代码实现了同样的功能——**体积缩小 99%**——在 600 MHz 核心上约 1 秒启动，**快了 400 倍**。

秘诀在哪？PicoClaw 95% 的核心代码由 AI 生成，人类只做审核和微调。AI 给自己写了一个超小的 AI 助手，属于是套娃了。

## 社区怎么看

不是所有人都买账。Hacker News 上 RISC-V 圈知名开发者 **brucehoult** 直接泼冷水："如果 OpenClaw 本身很重，那只是因为它是 JavaScript 跑在 Node.js 上"——言下之意，PicoClaw 的轻量不是什么新架构，而是换了语言。说得有道理，但结果摆在那里：4000 行 Go 代码，15 美元的板子上跑得起来。

分析师 Aakash Gupta 在 X 上看到了更大的故事："Sipeed 做 PicoClaw 跑在自家 9.9 美元的 LicheeRV Nano 上。写软件的公司同时生产硬件——这才是值得关注的。" 他接着说："PicoClaw 跑在 RISC-V 上，意味着 AI 代理运行在零西方 IP 授权依赖的芯片上。" 深圳系的软硬件垂直整合，这盘棋不小。

与此同时，一位 Hacker News 用户已经把 PicoClaw 移植到了一台 **2GB 内存的 32 位 Windows 笔记本**上——用 Claude 和 Copilot 辅助编码完成的。"提前声明：我不完全理解自己改了什么"，他写道。但它跑起来了。上线不到两周，PicoClaw 在 GitHub 上已经积累超过 **16,000 颗 star**，社区已经开始各种魔改了。

## 怎么上手

PicoClaw 完全开源，托管在 GitHub。可以直接下载预编译二进制文件，也能从源码编译。配置文件在 `~/.picoclaw/config.json`，指定 LLM 提供商并绑定你的账户就行。

推荐硬件是 15 美元左右的 Sipeed LicheeRV Nano，但只要有几十兆内存的设备都能跑——老款树莓派、各种 ARM 开发板、甚至淘汰的 x86 瘦客户机都没问题。社区已经有人在尝试接入本地 Ollama 实现完全离线运行。

## 为什么值得关注

AI 助手从纯云端向边缘端迁移的趋势一直在加速，但 PicoClaw 把这件事推向了极致。一个跑在一顿外卖钱硬件上的个人 AI 代理，内存占用比浏览器一个标签页还少——这种时候，「让每个人都用上 AI」的口号才真正有了实感。

有人直言不讳："10 美元的板子跑的是聊天客户端，AI 还是在云上。" 技术上确实如此——PicoClaw 推理还是要调外部 LLM。但正因为编排层可以这么轻，才打开了一个可能性：新兴市场的十亿台 IoT 设备，每台花不到 10 美元的硬件成本就能跑一个个人 AI 代理。这件事值得持续关注。

---

**来源：** <a href="https://www.cnx-software.com/2026/02/10/picoclaw-ultra-lightweight-personal-ai-assistant-run-on-just-10mb-of-ram/" target="_blank" rel="noopener noreferrer">CNX Software</a>
